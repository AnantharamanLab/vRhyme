#! /usr/bin/env python3
# vRhyme
# Author: Kristopher Kieft
# University of Wisconsin-Madison

import sys
import argparse
import os
import subprocess
from datetime import date
import logging
import resource
import time
import datetime
import pickle

descript = '''
\n\033[1m
#########################################################
#  vRhyme: Binning Virus Genomes from Metagenomes       #
#  Version 1.0.0 (December 2021)                        #
#  University of Wisconsin-Madison, Anantharaman Lab    #
#  Contact: Kristopher Kieft (kieft@wisc.edu)           #
#########################################################

 minimum input example with bam files:
 vRhyme -i fasta -b bam_folder/*.bam

 minimum input example with a coverage file:
 vRhyme -i fasta -c coverage_file.tsv

 full BAM input example:
 vRhyme -i fasta -g genes -p proteins -b bam_folder/*.bam -t threads -o output_folder/

 reads input example:
 vRhyme -i fasta -g genes -p proteins -r paired_reads_folder/*.fastq -t threads -o output_folder/ --method longest

 only use dereplicate function:
 vRhyme -i input_fasta -t threads -o output_folder/ --derep_only --method longest

___________________________________________________________________
\033[0m
'''
vRhyme = argparse.ArgumentParser(description=descript, formatter_class=argparse.RawTextHelpFormatter, usage=argparse.SUPPRESS)
vRhyme.add_argument('--version', action='version', version='vRhyme v1.0.0')
common = vRhyme.add_argument_group('COMMONLY USED')
inputs = vRhyme.add_argument_group('OTHER INPUTS')
outputs = vRhyme.add_argument_group('EDIT OUTPUTS')
read_align = vRhyme.add_argument_group('READ ALIGNMENT')
filtering = vRhyme.add_argument_group('BIN FILTERS')
dereplication = vRhyme.add_argument_group('DEREPLICATION')
#
common.add_argument('-i', type=str, nargs=1, default=[''], help='input nucleotide scaffolds')
common.add_argument('-o', type=str, nargs=1, default=[''], help="output folder [vRhyme_results_<input_scaffolds_name>]")
common.add_argument('-g', type=str, nargs=1, default = [''], help="input nucleotide genes in format name_#, skip to run Prodigal")
common.add_argument('-p', type=str, nargs=1, default = [''], help="input amino acid proteins in format name_#, skip to run Prodigal")
common.add_argument('-b', type=str, nargs='*', default = [''], help='BAM (.bam) sequence alignment files, sorted or unsorted.')
common.add_argument('-t', type=str, nargs=1, default=['5'], help="number of parallel processes to run (equivalent to threads, see --speed) [5]")
common.add_argument('-l', type=str, nargs=1, default=['2000'], help="minimum scaffold length to consider for binning [2000]")
#
inputs.add_argument('-s', type=str, nargs='*', default = [''], help='SAM (.sam) sequence alignment files.')
inputs.add_argument('-r', type=str, nargs='*', default = [''], help="paired forward (_1.fastq) and reverse (_2.fastq) read files")
inputs.add_argument('-u', type=str, nargs='*', default = [''], help="unpaired (.fastq) read files")
inputs.add_argument('-v', type=str, nargs='*', default = [''], help="interleaved paired (.fastq) read files, '--aligner bowtie2' only")
inputs.add_argument('-c', type=str, nargs=1, default = [''], help="coverage table generated from vRhyme or from cov_table_convert.py")
inputs.add_argument('--interest', metavar='', type=str, nargs=1, default = [''], help="file containing a list of scaffolds of interest to dereplicate/bin.")
#
outputs.add_argument('--keep_sam', action='store_true', help="do not remove SAM files generated by vRhyme, reads input only [off].")
outputs.add_argument('--keep_bam', action='store_true', help="do not remove unsorted/index BAM files generated by vRhyme, always keep sorted [off].")
outputs.add_argument('--speed', action='store_true', help="allow extra CPU multithreading per process (may use more CPUs than '-t <int>' per process) [off].")
outputs.add_argument('--verbose', action='store_true', help="write log file information to standard out while running [off].")
outputs.add_argument('--prefix', metavar='', type=str, nargs=1, default=['vRhyme_#__'], help="prefix to append to binned scaffold names, use the '#' symbol where the bin number will be noted [vRhyme_#__]")
#
dereplication.add_argument('--derep_only', action='store_true', help="only use dereplication function, skip binning [off].")
dereplication.add_argument('--method', type=str, nargs=1, choices=['composite', 'longest', 'none'], default = ['none'], help="method of input scaffold dereplication, 'composite' compatible with reads input or --derep_only [none].")
dereplication.add_argument('-m', metavar='', type=str, nargs=1, default=['1000'], help="minimum scaffold length to consider for dereplication [1000]")
dereplication.add_argument('--derep_id', metavar='', type=str, nargs=1, default = [''], help="minimum percent similarity for dereplication [composite=0.99, longest=0.97].")
dereplication.add_argument('--frac', metavar='', type=str, nargs=1, default = [''], help='minimum coverage in overlap percent for dereplication [composite=0.20, longest=0.70].')
dereplication.add_argument('--mash_k', metavar='', type=str, nargs=1, default = ['31'], help='equivalent to mash sketch -k [31].')
dereplication.add_argument('--mash_s', metavar='', type=str, nargs=1, default = ['1000'], help='equivalent to mash sketch -s [1000].')
dereplication.add_argument('--sens_ends', metavar='', type=str, nargs=1, default = ['0.10'], help="free ends sensitivity, as fraction of scaffold length, to merging over regions of complexity; requires '--method composite' [0.10].")
dereplication.add_argument('--sens_overlap', metavar='', type=str, nargs=1, default = ['0.25'], help="alignment overlap sensitivity, as fraction of scaffold length, to merging over regions of complexity; requires '--method composite' [0.25].")
dereplication.add_argument('--nucmer_c', metavar='', type=str, nargs=1, default = ['1000'], help='equivalent to nucmer -c [1000].')
dereplication.add_argument('--nucmer_b', metavar='', type=str, nargs=1, default = ['1000'], help='equivalent to nucmer -b [1000].')
dereplication.add_argument('--nucmer_g', metavar='', type=str, nargs=1, default = ['1000'], help='equivalent to nucmer -g [1000].')
dereplication.add_argument('--nuc_split', metavar='', type=str, nargs=1, default = ['3'], help='maximum fragmentations allowed per scaffold in nucmer alignment [3].')
#
read_align.add_argument('--aligner', type=str, nargs=1, choices = ['bowtie2', 'bwa'], default = ['bowtie2'], help="read alignment software to use [bowtie2]")
read_align.add_argument('--read_id', metavar='', type=str, nargs=1, default = ['0.97'], help="minimum percent identity per aligned read for calculating coverage [0.97].")
read_align.add_argument('--mask', metavar='', type=str, nargs=1, default=['150'], help="mask coverage values <int> bases on each end of a scaffold [150]")
#
filtering.add_argument('--bin_size', metavar='', type=str, nargs=1, default=['2'], help="minimum number of scaffolds per bin [2]")
filtering.add_argument('--iter', metavar='', type=str, nargs=1, default=['10'], help="number of binning iterations (presets) to run [10, range: 8-15]")
filtering.add_argument('--red', metavar='', type=str, nargs=1, default=['50'], help="maximum number of redundant proteins per bin [50]")
filtering.add_argument('--cov', metavar='', type=str, nargs=1, default=['0.90'], help="coverage threshold to consider scaffold as present in sample [0.90]")
filtering.add_argument('--model', type=str, nargs=1, choices=['hybrid', 'NN', 'ET'], default=['hybrid'], help="machine learning model(s) to use [hybrid]")
filtering.add_argument('--max_gc', metavar='', type=str, nargs=1, default=['0.20'], help="maximum gc distance for pre-filtering [0.20]")
filtering.add_argument('--min_kmer', metavar='', type=str, nargs=1, default=['0.60'], help="minimum k-mer distance for pre-filtering [0.60]")
filtering.add_argument('--max_edges', metavar='', type=str, nargs=1, default=['6'], help="maximum number of edges per node in network clustering [6]")
filtering.add_argument('--penalty_w', metavar='', type=str, nargs=1, default=['0.20'], help="penalty weight for Cohen's d distance calculations [0.20]")
filtering.add_argument('--penalty_n', metavar='', type=str, nargs=1, default=['2'], help="maximum number of penalties for Cohen's d distance calculations [2]")
filtering.add_argument('--mems', metavar='', type=str, nargs=1, default=['4'], help="refine bins with at least <int> members [4]")

args = vRhyme.parse_args()

if args.speed == False:
    # 1 CPU per process unless iterations < threads (add int remainder if >= 1 across all processes)
    mp_env = 0
    mp_env += int(int(args.t[0])/int(args.iter[0]))
    if mp_env == 0:
        mp_env = 1
    os.environ["OMP_NUM_THREADS"] = f'{mp_env}'
    os.environ["OPENBLAS_NUM_THREADS"] = f'{mp_env}'
    os.environ["MKL_NUM_THREADS"] = f'{mp_env}'
    os.environ["VECLIB_MAXIMUM_THREADS"] = f'{mp_env}'
    os.environ["NUMEXPR_NUM_THREADS"] = f'{mp_env}'

try:
    from scripts import save_dist_dicts as info
    from scripts import linclust_stuff
    from scripts import machine_stuff
    from scripts import distance_stuff
    from scripts import read_nt_stuff
    from scripts import kmer_nt_stuff
    from scripts import codon_stuff
    from scripts import bowtie2_stuff
    from scripts import bwa_stuff
    from scripts import extract_stuff
    from scripts import coverage_stuff
    from scripts import derep_stuff
    from scripts import prodigal_stuff
    from scripts import score_stuff
except Exception as e:
    sys.stderr.write("\nError: could not load dependencies. Try running test_vRhyme.py.\n\n")
    sys.stderr.write(str(e) + "\n")
    exit(1)

def looper(s_set):
    while True:
        for item in s_set:
            p = item.poll()
            if p != None:
                return item
        time.sleep(0.1)

vRhyme_path = str(os.path.dirname(os.path.abspath(__file__)))
if os.path.exists(f'{vRhyme_path}/models/vRhyme_machine_model_ET.sav.gz'):
    # first time running vRhyme, unzip the larger ET model
    subprocess.run(f'gunzip {vRhyme_path}/models/vRhyme_machine_model_ET.sav.gz', shell=True)
#
starting_time = time.time()
start_time = str(datetime.datetime.now().time()).rsplit(".",1)[0]
#
in_fasta = str(args.i[0])
try:
    temp = in_fasta.rsplit("/",1)[1]
    base = temp.rsplit(".",1)[0]
except Exception:
    base = in_fasta.rsplit(".",1)[0]
in_prots = str(args.p[0])
in_genes = str(args.g[0])
prefix = str(args.prefix[0])
min_len = int(args.l[0])
model_method = str(args.model[0])
refine_m = int(args.mems[0])
red_cutoff = int(args.red[0])
if red_cutoff < 0:
    sys.stderr.write(f"\nError: input --red as a positive value. Exiting.\n")
    exit(1)
iterations = int(args.iter[0])
if iterations < 8 or iterations > 15:
    sys.stderr.write("\nError: --iter must be between 8 and 15. Exiting.\n")
    exit(1)
bin_size = int(args.bin_size[0])
max_edges = int(args.max_edges[0])
max_gc = float(args.max_gc[0])
min_kmer = float(args.min_kmer[0])
penalty_w = float(args.penalty_w[0])
penalty_n = int(args.penalty_n[0])
derep_len = int(args.m[0])
mask = int(args.mask[0])
min_cov = float(args.cov[0])
threads = str(args.t[0])
if threads == '0':
    sys.stderr.write("\nError: threads must be an integer > 0. Exiting.\n")
    exit(1)
try:
    check = int(threads)
except ValueError:
    sys.stderr.write("\nError: threads must be an integer > 0. Exiting.\n")
    exit(1)
interest = str(args.interest[0])
cov_table = str(args.c[0])
paired_files = list(args.r)
single_files = list(args.u)
interleaved_files = list(args.v)
sam_files = list(args.s)
bam_files = list(args.b)
aligner = str(args.aligner[0])
derep_method = str(args.method[0])
sketch_k = int(args.mash_k[0])
sketch_s = int(args.mash_s[0])
nuc_c = int(args.nucmer_c[0])
nuc_b = int(args.nucmer_b[0])
nuc_g = int(args.nucmer_g[0])
nuc_split = int(args.nuc_split[0])
sens1 = float(args.sens_ends[0])
sens2 = float(args.sens_overlap[0])

if args.derep_only == True and derep_method == 'none':
    sys.stderr.write("\nError: --derep_only and --method 'none' cannot be set together. Exiting.\n")
    exit(1)

if (paired_files != [''] or single_files != [''] or interleaved_files != [''] or sam_files != [''] or bam_files != ['']) and cov_table:
    sys.stderr.write("\nError: coverage table (-c) input is not compatible with -r/-u/-v/-b/-s. Exiting.\n")
    exit(1)

if sens1 > 0.30 or sens2 > 0.50:
    sys.stderr.write("\nError: high sensitivity alignment filtering should not exceed values of 0.30/0.50 for sens_ends/sens_overlap. Exiting.\n")
    exit(1)

if args.derep_id[0] == '':
    if derep_method == 'composite':
        derep_id = 0.99
    elif derep_method == 'longest':
        derep_id = 0.97
    elif derep_method == 'none':
        derep_id = 0
else:
    derep_id = float(args.derep_id[0])

if args.frac[0] == '':
    if derep_method == 'composite':
        derep_frac = 0.2
    elif derep_method == 'longest':
        derep_frac = 0.7
    elif derep_method == 'none':
        derep_frac = 0
else:
    derep_frac = float(args.frac[0])

if float(args.read_id[0]) > 1:
    sys.stderr.write(f"\nError: input --read_id as a decimal value. Example, input '--read_id 0.97' for 97%. Exiting.\n")
    exit(1)

read_id = 1.0 - float(args.read_id[0])

if in_genes != '' and not os.path.exists(in_genes):
    sys.stderr.write("\nError: file does not exist: -g %s. Exiting.\n" % in_genes)
    exit(1)
if in_prots != '' and not os.path.exists(in_prots):
    sys.stderr.write("\nError: file does not exist: -p %s. Exiting.\n" % in_prots)
    exit(1)

if derep_id > 1 or derep_frac > 1:
    sys.stderr.write("\nError: 'derep_id' and 'derep_frac' should be decimal values below 1 (100%). Exiting.\n")
    exit(1)

if min_len < 2000:
    sys.stderr.write("\nError: minimum scaffold length (-l %s) cannot be set below 2000 for binning. Exiting.\n" % min_len)
    exit(1)
if derep_len < 1000:
    sys.stderr.write("\nError: minimum scaffold length (-m %s) cannot be set below 1000 for dereplication. Exiting.\n" % derep_len)
    exit(1)
folder = str(args.o[0])
if folder == '':
    folder = 'vRhyme_results_' + str(in_fasta).rsplit(".",1)[0]
if folder[-1] != "/":
    folder += "/"

if in_fasta == '':
    sys.stderr.write("\nError: input fasta file sequences (-i) required. Exiting.\n")
    exit(1)

if os.path.exists(folder):
    sys.stderr.write("\nError: the output folder (-o) already exists (%s). Remove the existing folder or set a different output folder. Exiting.\n" % folder)
    exit(1)
else:
    subprocess.run('mkdir ' + folder, shell=True)

####################################


if base != '':
    logfilename = folder + "log_vRhyme_" + base + ".log"
else:
    logfilename = folder + "log_vRhyme.log"
subprocess.run("rm " + logfilename + " 2> /dev/null", shell=True)


file_handler = logging.FileHandler(filename=logfilename)
handlers = [file_handler]
if args.verbose == True:
    stdout_handler = logging.StreamHandler(sys.stdout)
    handlers.append(stdout_handler)

logging.basicConfig(handlers=handlers, level=logging.INFO, format='%(message)s')

logging.info("Command:  " + str(" ".join(sys.argv)))
logging.info("")
logging.info("Date:     %s (y-m-d)" % date.today())
logging.info("Start:    %s   (h:m:s)" % start_time)
logging.info("Program:  vRhyme v1.0.0")
logging.info("\n")
logging.info("Time (min) |  Log                                                   ")
logging.info("--------------------------------------------------------------------")
log_time = str(round((time.time() - float(starting_time))/60,2))
blanks = ''.join([' '] * (14-len(log_time)))
logging.info("%s%sInitializing and validating vRhyme parameters" % (log_time,blanks))

###################################

# (ml, cohen, net)
# (machine learning, cohen's d, network edge)
presets = [
(0.4, 0.6, 0.6),        #0
(0.5, 0.6, 0.6),        #1
(0.5, 0.7, 0.65),       #2
(0.425, 0.625, 0.625),  #3
(0.7, 0.7, 0.7),        #4
(0.6, 0.6, 0.6),        #5
(0.4, 0.4, 0.65),       #6
(0.45, 0.65, 0.65),     #7
(0.425, 0.525, 0.625),  #8
(0.45, 0.5, 0.65),      #9
(0.5, 0.5, 0.5),        #10
(0.4, 0.4, 0.4),        #11
(0.525, 0.725, 0.625),  #12
(0.525, 0.525, 0.525),  #13
(0.7, 0.6, 0.5),        #14
]

possible_cohens = [pc[1] for pc in presets]
max_cohen = max(possible_cohens)

distances = None
binned_seqs = 0
bins_count = 0
binned_prots = 0
binned_redundancy = 0
final = 'none'
best_score = 'none'

if args.derep_only == False: #comes before derep to check read file status before proceeding
    subprocess.run('rm -R ' + folder + "vRhyme_sam_files " + folder + "vRhyme_bam_files " + folder + "vRhyme_coverage_files 2> /dev/null", shell=True)
    time.sleep(0.1)
    subprocess.Popen('mkdir ' + folder + "vRhyme_sam_files", shell=True)
    subprocess.Popen('mkdir ' + folder + "vRhyme_bam_files", shell=True)
    subprocess.Popen('mkdir ' + folder + "vRhyme_coverage_files", shell=True)


    try:
        subprocess.check_output("which mmseqs", shell=True)
    except Exception:
        logging.info("\nError: mmseqs2 cannot be found. Please install mmseqs2. Exiting.")
        exit(1)

    import sklearn
    sk_version = sklearn.__version__
    sk = sk_version.split('.')
    if len(sk) < 3:
        logging.info(f"\nError: scikit-learn version {sk_version} is not compatible with vRhyme. Please update to version >= 0.23.0. Exiting.")
        exit(1)
    elif int(sk[1]) < 23 and sk[0] == '0': # version >= 1.0.0 is compatible
        logging.info(f"\nError: scikit-learn version {sk_version} is not compatible with vRhyme. Please update to version >= 0.23.0. Exiting.")
        exit(1)


    #########################
    if in_genes == '' or in_prots == '':
        try:
            subprocess.check_output("which prodigal", shell=True)
        except Exception:
            logging.info("Error: Prodigal cannot be found. Install Prodigal or input predicted genes/proteins. Exiting.")
            exit(1)
    # check reads and generate reads list

    total_sets = 0
    if paired_files != ['']:
        subprocess.run("rm " + folder + "log_vRhyme_paired_reads.log 2> /dev/null", shell=True)
        with open(folder + "log_vRhyme_paired_reads.tsv", 'w') as pairedwrite:
            pairedwrite.write(f'forward\treverse\n')
            paired_files.sort()
            for n in range(0,len(paired_files),2):
                pairedwrite.write(str(paired_files[n]) + "\t" + str(paired_files[n+1]) + "\n")
                if (paired_files[n].endswith("_1.fastq") or paired_files[n].endswith("_1.fastq.gz")) and (paired_files[n+1].endswith("_2.fastq") or paired_files[n+1].endswith("_2.fastq.gz")):
                    total_sets += 1
                elif (paired_files[n].endswith("_R1.fastq") or paired_files[n].endswith("_R1.fastq.gz")) and (paired_files[n+1].endswith("_R2.fastq") or paired_files[n+1].endswith("_R2.fastq.gz")):
                    total_sets += 1
                else:   
                    sys.stderr.write("\nError: matching paried reads not identified. Use suffix terminology _1.fastq/_R1.fastq for forward and _2.fastq/_R2.fastq for reverse. Files may be in gzip (fastq.gz) format. Exiting.\n")
                    logging.info("\nError: matching paried reads not identified. Use suffix terminology _1.fastq/_R1.fastq for forward and _2.fastq/_R2.fastq for reverse. Files may be in gzip (fastq.gz) format. Exiting.\n")
                    exit(1)

    if single_files != ['']:
        for item in single_files:
            if item.endswith(".fastq") or item.endswith(".fastq.gz"):
                total_sets += 1
            else:   
                sys.stderr.write("\nError: unpaired reads not identified. Use extension .fastq. Files may be in gzip (fastq.gz) format. Exiting.\n")
                logging.info("\nError: unpaired reads not identified. Use extension .fastq. Files may be in gzip (fastq.gz) format. Exiting.\n")
                exit(1)

    if interleaved_files != ['']:
        for item in interleaved_files:
            if item.endswith(".fastq") or item.endswith(".fastq.gz"):
                total_sets += 1
            else:   
                sys.stderr.write("\nError: interleaved reads not identified. Use extension .fastq. Files may be in gzip (fastq.gz) format. Exiting.\n")
                logging.info("\nError: interleaved reads not identified. Use extension .fastq. Files may be in gzip (fastq.gz) format. Exiting.\n")
                exit(1)

    if total_sets == 0 and derep_method == 'composite' and args.derep_only == False:
        sys.stderr.write("\nError: 'composite' dereplication is only compatible with reads input or --derep_only is set. Exiting.\n")
        logging.info("\nError: 'composite' dereplication is only compatible with reads input or --derep_only is set. Exiting.\n")
        exit(1)

    if total_sets > 0:
        if aligner == 'bowtie2':
            try:
                subprocess.check_output("which bowtie2", shell=True)
            except Exception:
                logging.info("Error: Bowtie2 cannot be found. Install Bowtie2, try --aligner bwa or align read separately. Exiting.")
                exit(1)
        elif aligner == 'bwa':
            try:
                subprocess.check_output("which bwa", shell=True)
            except Exception:
                logging.info("Error: BWA cannot be found. Install BWA, try --aligner bowtie2 or align read separately. Exiting.")
                exit(1)

#########################

# dereplicatiton
if derep_method != 'none':
    subprocess.Popen('rm -R ' + folder + "vRhyme_dereplication 2> /dev/null", shell=True)
    time.sleep(0.1)
    subprocess.Popen('mkdir ' + folder + "vRhyme_dereplication", shell=True)
    interest_list = None
    if interest != '':
        with open(interest, 'r') as listfile:
            try:
                interest_list = listfile.read().split("\n")
            except Exception:
                sys.stderr.write("\nError: scaffolds of interest file (--interest) must be a new-line separated list of sequence names. Exiting.\n")
                logging.info("\nError: scaffolds of interest file (--interest) must be a new-line separated list of sequence names. Exiting.\n")
                exit(1)
        len_interest = len(interest_list)
        if len_interest < 2:
            sys.stderr.write("\nError: %s scaffolds in '--interest' were found. Requires at least 25. Exiting.\n" % str(len_interest))
            logging.info("\nError: %s scaffolds in '--interest' were found. Requires at least 25. Exiting.\n" % str(len_interest))
            exit(1)

        interest_list = set(interest_list)

    if derep_method == 'composite':
        log_time = str(round((time.time() - float(starting_time))/60,2))
        blanks = ''.join([' '] * (14-len(log_time)))
        logging.info("%s%sRunning 'composite' dereplication: %d%% identity and %d%% coverage" % (log_time, blanks, derep_id*100, derep_frac*100))
        code, original, sequences = derep_stuff.derep_stuff_composite(in_fasta, int(threads), interest_list, folder, base, derep_len, nuc_c, nuc_b, nuc_g, nuc_split, sens1, sens2, derep_id, derep_frac, sketch_k, sketch_s, iterations)
    elif derep_method == 'longest':
        log_time = str(round((time.time() - float(starting_time))/60,2))
        blanks = ''.join([' '] * (14-len(log_time)))
        logging.info("%s%sRunning 'longest' dereplication: %d%% identity and %d%% coverage" % (log_time, blanks, derep_id*100, derep_frac*100))
        code, original, sequences = derep_stuff.derep_stuff_longest(in_fasta, int(threads), interest_list, folder, base, derep_len, nuc_c, nuc_b, nuc_g, nuc_split, derep_id, derep_frac, sketch_k, sketch_s, iterations)

    if code == 'finished':
        log_time = str(round((time.time() - float(starting_time))/60,2))
        blanks = ''.join([' '] * (14-len(log_time)))
        logging.info("%s%sDereplication finished. %d sequences were dereplicated to %d" % (log_time, blanks,original,sequences))
        if derep_method == 'composite':
            in_fasta = folder + 'vRhyme_dereplication/vRhyme_derep_composite_' + base + '.fa'
            base = base + '.derep_composite'
        else:
            in_fasta = folder + 'vRhyme_dereplication/vRhyme_derep_longest_' + base + '.fa'
            base = base + '.derep_longest'

    elif code == 'error1':
        new_fasta = "vRhyme_dereplication/" + base + '.vRhyme-unique.fa'
        logging.info("\nError: duplicate sequence names found. %s were of sufficient length (%sbp) but only %s were unique.\n" % (sequences,derep_len,original))
        logging.info("Unique names were written to %s. Exiting.\n" % new_fasta)
        exit()
    elif code == 'noalign':
        log_time = str(round((time.time() - float(starting_time))/60,2))
        blanks = ''.join([' '] * (14-len(log_time)))
        logging.info("%s%sNo sequences were of sufficient similarity to dereplicate" % (log_time,blanks))
        derep_method == '' # empty for Prodigal running

    interest = '' # if derep interest then skip main interest

if args.derep_only == True:
    mem_self = float(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)
    mem_child = float(resource.getrusage(resource.RUSAGE_CHILDREN).ru_maxrss)
    max_mem = float(max([mem_self, mem_child])/1e6)
    mem = round(max_mem,2)
    log_time = str(round((time.time() - float(starting_time))/60,2))
    blanks = ''.join([' '] * (14-len(log_time)))
    logging.info("%s%sFinished.\n" % (log_time,blanks))
    logging.info("\n")
    logging.info("Memory usage:            %s" % mem)
    logging.info("Runtime (min):           %s" % log_time)
    logging.info("Original sequences:      %s" % original)
    logging.info("Dereplicated sequences:  %s" % sequences)
    exit() # vRhyme finished

#########################

if interest != '':
    log_time = str(round((time.time() - float(starting_time))/60,2))
    blanks = ''.join([' '] * (14-len(log_time)))
    logging.info("%s%sExtracting scaffolds of interest" % (log_time,blanks))
    with open(interest, 'r') as listfile:
        try:
            interest_list = listfile.read().split("\n")
        except Exception:
            sys.stderr.write("\nError: scaffolds of interest file (--interest) must be a new-line separated list of sequence names. Exiting.\n")
            logging.info("\nError: scaffolds of interest file (--interest) must be a new-line separated list of sequence names. Exiting.\n")
            exit(1)
    len_interest = len(interest_list)
    if len_interest < 2:
        sys.stderr.write("\nError: %s scaffolds in '--interest' were found. Requires at least 25. Exiting.\n" % str(len_interest))
        logging.info("\nError: %s scaffolds in '--interest' were found. Requires at least 25. Exiting.\n" % str(len_interest))
        exit(1)

    interest_list = set(interest_list)
    keep, spaces, mapper = read_nt_stuff.read_nt_stuff_interest(in_fasta, min_len, interest_list)
    interest_list = None

else:
    keep, spaces, mapper = read_nt_stuff.read_nt_stuff(in_fasta, min_len)


length_keep = len(keep.keys())
if length_keep < 2:
    sys.stderr.write("\nError: %s scaffolds at least %s bp were found. Requires at least 2. Exiting.\n" % (str(length_keep),str(min_len)))
    logging.info("\nError: %s scaffolds at least %s bp were found. Requires at least 2. Exiting.\n" % (str(length_keep),str(min_len)))
    exit(1)


##########
bam_raw = []
if interleaved_files != ['']:
    log_time = str(round((time.time() - float(starting_time))/60,2))
    blanks = ''.join([' '] * (14-len(log_time)))
    logging.info("%s%sInterleaved paired end read file(s) identified. Running %s on %d paired files" % (log_time,blanks,aligner,len(interleaved_files)))
    for single in interleaved_files:
        if aligner == 'bowtie2':
            build = bowtie2_stuff.bowtie2_build_stuff(in_fasta, spaces, folder)
            bam = bowtie2_stuff.bowtie2_interleaved_stuff(threads, single, folder, build, args.keep_sam, spaces)
            bam_raw.append(bam)
        else:
            sys.stderr.write("\nError. Interleaved paired reads format is not compatible with --aligner bwa. Exiting.\n")
            logging.info("\nError. Interleaved paired reads format is not compatible with --aligner bwa. Exiting.\n")
            exit(1)

if paired_files != ['']:
    log_time = str(round((time.time() - float(starting_time))/60,2))
    blanks = ''.join([' '] * (14-len(log_time)))
    logging.info("%s%sPaired end read file(s) identified. Running %s on %d files" % (log_time,blanks,aligner,len(paired_files)/2))
    for n in range(0,len(paired_files),2):
        forward = paired_files[n]
        reverse = paired_files[n+1]
        if aligner == 'bowtie2':
            build = bowtie2_stuff.bowtie2_build_stuff(in_fasta, spaces, folder)
            bam = bowtie2_stuff.bowtie2_paired_stuff(threads, forward, reverse, folder, build, args.keep_sam, spaces)
            bam_raw.append(bam)
        elif aligner == 'bwa':
            build = bwa_stuff.bwa_build_stuff(in_fasta, spaces, folder)
            bam = bwa_stuff.bwa_paired_stuff(threads, forward, reverse, folder, build, args.keep_sam, spaces)
            bam_raw.append(bam)

if single_files != ['']:
    log_time = str(round((time.time() - float(starting_time))/60,2))
    blanks = ''.join([' '] * (14-len(log_time)))
    logging.info("%s%sSingle end read file(s) identified. Running %s on %d files" % (log_time,blanks,aligner,len(single_files)))
    for single in single_files:
        if aligner == 'bowtie2':
            build = bowtie2_stuff.bowtie2_build_stuff(in_fasta, spaces, folder)
            bam = bowtie2_stuff.bowtie2_single_stuff(threads, single, folder, build, args.keep_sam, spaces)
            bam_raw.append(bam)
        elif aligner == 'bwa':
            build = bwa_stuff.bwa_build_stuff(in_fasta, spaces, folder)
            bam = bwa_stuff.bwa_single_stuff(threads, single, folder, build, args.keep_sam, spaces)
            bam_raw.append(bam)

try:
    subprocess.run(f"rm {build}* 2> /dev/null", shell=True) # build may not exist
    subprocess.run(f"rm {folder}{base}.no-spaces.fasta 2> /dev/null", shell=True)
    subprocess.run(f"rm {folder}vRhyme_derep*{base}.no-spaces.fasta 2> /dev/null", shell=True)
except Exception:
    pass

###### input is sam
if sam_files != ['']:
    log_time = str(round((time.time() - float(starting_time))/60,2))
    blanks = ''.join([' '] * (14-len(log_time)))
    logging.info("%s%sProcessing SAM files" % (log_time,blanks))

    s_set = set()
    opened = 0
    for f in sam_files:
        if f == '':
            continue
        try:
            temp = f.rsplit("/",1)[1]
            base = temp.rsplit(".",1)[0]
        except Exception:
            base = f.rsplit(".",1)[0]
        bam = f'{folder}vRhyme_bam_files/{base}.bam'
        if os.path.exists(bam): # don't overwrite a file
            bam = f'{folder}vRhyme_bam_files/{base}.version2.bam'
        bam_raw.append(bam)
        s = subprocess.Popen(f'{vRhyme_path}/scripts/sam_stuff.py {f} {bam}', shell=True)
        s_set.add(s)
        opened += 1
        if opened == int(threads):
            item = looper(s_set)
            opened -= 1
            s_set.remove(item)
    for item in s_set:
        item.wait()

##### input is bam
for bam in bam_files:
    if bam != '':
        bam_raw.append(bam)

# bam_raw holds all bams from all inputs

if len(bam_raw) < 3:
        logging.info("              Caution: vRhyme performs optimally with 3+ samples")
#####
if len(bam_raw) == 0 and cov_table == '':
    sys.stderr.write("\nError: Zero BAM/SAM files (-b/-s) or no coverage table (-c) was identified. Exiting.\n")
    logging.info("\nError: Zero BAM/SAM files (-b/-s) or no coverage table (-c) was identified. Exiting.\n")
    exit(1)

##### bam filtering and coverage calculations
subprocess.run("rm " + folder + base + ".coverage.tsv 2> /dev/null", shell=True)
subprocess.run("rm " + folder + base + ".cohen-d.tsv 2> /dev/null", shell=True)
subprocess.run("rm " + folder + "log_vRhyme_no-alignments.log 2> /dev/null", shell=True)

with open(folder + "keep_pickle.sav", 'wb') as save_keep:
    pickle.dump(keep, save_keep)

keep_set = set(keep.keys())
if len(bam_raw) != 0:
    log_time = str(round((time.time() - float(starting_time))/60,2))
    blanks = ''.join([' '] * (14-len(log_time)))
    logging.info("%s%sExtracting coverage information from BAM files" % (log_time,blanks))
    bam_remove = []
    cohen_check = len(bam_raw) # number of cohen d values to expect
    s_set = set()
    opened = 0
    for f in bam_raw:
        if f == '':
            continue
        s = subprocess.Popen(f'{vRhyme_path}/scripts/coverage_stuff_threader.py {f} {mask} {folder} {read_id}', shell=True)
        s_set.add(s)
        opened += 1
        if opened == int(threads):
            item = looper(s_set)
            opened -= 1
            s_set.remove(item)
    for item in s_set:
        item.wait()

    if args.keep_bam == False:
        s_set = set()
        opened = 0
        files = os.listdir(f'{folder}vRhyme_bam_files')
        files = [f for f in files if not f.endswith('sorted.bam')]
        for f in files:
            s = subprocess.Popen(f'rm {folder}vRhyme_bam_files/{f}', shell=True)
            s_set.add(s)
            opened += 1
            if opened == int(threads):
                item = looper(s_set)
                opened -= 1
                s_set.remove(item)
        for item in s_set:
            item.wait()

    if os.path.exists(folder + "log_vRhyme_bam_noalign.txt"):
        with open(folder + "log_vRhyme_bam_noalign.txt", "r") as temp:
            bam_remove = temp.read().split("\n")
    if cohen_check == 0 or len(bam_raw) - len(bam_remove) == 0:
        logging.info("\nError. No reads were aligned from any sample (all zero coverage). Exiting.\n\n")
        sys.stderr.write("\nError. No reads were aligned from any sample (all zero coverage). Exiting.\n\n")
        exit(1)

    # generate some useful lists/files
    with open(folder + "vRhyme_coverage_files/vRhyme_names.txt", 'w') as names:
        names.write("scaffold")
        for name in keep.keys():
            names.write("\n" + name)

    log_time = str(round((time.time() - float(starting_time))/60,2))
    blanks = ''.join([' '] * (14-len(log_time)))
    logging.info("%s%sCoverage extraction complete. Generating coverage table" % (log_time,blanks))
    subprocess.run("paste " + folder + "vRhyme_coverage_files/vRhyme_names.txt " + folder + "vRhyme_coverage_files/*coverage.tsv > " + folder + "vRhyme_coverage_files/vRhyme_coverage_values.tsv", shell=True)
    cov_table = folder + "vRhyme_coverage_files/vRhyme_coverage_values.tsv" # else cov_table is input -c

log_time = str(round((time.time() - float(starting_time))/60,2))
blanks = ''.join([' '] * (14-len(log_time)))
logging.info("%s%sPerforming pairwise coverage comparisons" % (log_time,blanks))
keep_set = None
pair_genomes, pairs, cohen_list = coverage_stuff.coverage_table_stuff(cov_table, keep, max_cohen, min_cov, penalty_w, penalty_n, max_edges) 

if len(pair_genomes) > 1:
    prodigal_split = False
    subprocess.run("rm -R " + folder + "vRhyme_split_runs 2> /dev/null", shell=True)
    if in_genes == '' or in_prots == '':
        log_time = str(round((time.time() - float(starting_time))/60,2))
        blanks = ''.join([' '] * (14-len(log_time)))
        logging.info("%s%sRunning Prodigal on filtered sequences" % (log_time,blanks))
        extract_stuff.split_stuff(in_fasta, folder, threads, pair_genomes, keep)
        in_prots, in_genes = prodigal_stuff.prodigal_stuff(in_fasta, folder, base, spaces, False)
        data_dict_codon = {}
        ffn_files = os.listdir(folder + "vRhyme_split_runs")
        s_list = []
        n = 0
        for ffn in ffn_files:
            f = ffn.rsplit(".",1)
            if f[1] == 'ffn':
                ffn = folder + "vRhyme_split_runs/" + ffn
                n += 1
                s = subprocess.Popen(f'{vRhyme_path}/scripts/codon_stuff_threader.py {ffn} {folder} vRhyme_split_runs/{n} {spaces}', shell=True)
                s_list.append(s)
        for item in s_list:
            item.wait()

        for i in range(1,n+1):
            with open(folder + "vRhyme_split_runs/" + str(i) + "_pickle.sav", 'rb') as read_i:
                codon = pickle.load(read_i)

            info.data_dict_codon.update(codon)
            codon = None
            subprocess.run("rm " + folder + "vRhyme_split_runs/" + str(i) + "_pickle.sav 2> /dev/null", shell=True)

        prodigal_split = True
        subprocess.Popen("rm " + folder + "vRhyme_split_runs/*.ffn 2> /dev/null", shell=True)
    elif derep_method == 'composite':
        log_time = str(round((time.time() - float(starting_time))/60,2))
        blanks = ''.join([' '] * (14-len(log_time)))
        logging.info("%s%sRunning Prodigal on composite sequences generated from dereplication" % (log_time,blanks))
        extract_stuff.split_stuff_composite(in_fasta, folder, threads, pair_genomes, mapper)
        in_genes_composites, in_prots_composites = prodigal_stuff.prodigal_stuff(in_fasta, folder, base, spaces, True)
        subprocess.run("cat " + in_genes + " " + in_genes_composites + " > " + folder + base + '.vRhyme_genes.ffn', shell=True)
        subprocess.run("cat " + in_prots + " " + in_prots_composites + " > " + folder + base + '.vRhyme_proteins.faa', shell=True)
        in_genes = folder + base + '.vRhyme_genes.ffn'
        in_prots = folder + base + '.vRhyme_proteins.faa'
        subprocess.run("rm -R " + folder + "vRhyme_split_runs/ 2> /dev/null", shell=True) # remove for kmer below


    log_time = str(round((time.time() - float(starting_time))/60,2))
    blanks = ''.join([' '] * (14-len(log_time)))
    logging.info("%s%sGenerating codon usage features" % (log_time,blanks))
    if int(threads) == 1 and prodigal_split == False:
        codon_stuff.codon_stuff(in_genes, pair_genomes, keep)
    elif int(threads) != 1 and prodigal_split == False:
        extract_stuff.split_genes_stuff(in_genes, folder, pair_genomes, keep, threads)
        spaces = 'no_keep'
        data_dict_codon = {}
        ffn_files = os.listdir(folder + "vRhyme_split_runs")
        s_list = []
        n = 0
        for ffn in ffn_files:
            f = ffn.rsplit(".",1)
            if f[1] == 'ffn':
                ffn = folder + "vRhyme_split_runs/" + ffn
                n += 1
                s = subprocess.Popen(f'{vRhyme_path}/scripts/codon_stuff_threader.py {ffn} {folder} vRhyme_split_runs/{n} {spaces}', shell=True)

                s_list.append(s)
        for item in s_list:
            item.wait()

        for i in range(1,n+1):
            with open(folder + "vRhyme_split_runs/" + str(i) + "_pickle.sav", 'rb') as read_i:
                codon = pickle.load(read_i)

            info.data_dict_codon.update(codon)
            codon = None
            subprocess.run("rm " + folder + "vRhyme_split_runs/" + str(i) + "_pickle.sav 2> /dev/null", shell=True)

    log_time = str(round((time.time() - float(starting_time))/60,2))
    blanks = ''.join([' '] * (14-len(log_time)))
    logging.info("%s%sGenerating nucleotide features" % (log_time,blanks))

    if int(threads) == 1:
        kmer_nt_stuff.kmer_nt_stuff(in_fasta, pair_genomes, keep)
    else:
        # multithread
        if prodigal_split == False: # not already split
            extract_stuff.kmer_split_stuff(in_fasta, folder, threads, pair_genomes, keep)

        data_dict_gc = {}
        data_dict_cpg = {}
        data_dict_skew = {}
        data_dict_zom = {}
        data_dict_tud = {}
        data_dict_nt = {}

        fasta_files = os.listdir(folder + "vRhyme_split_runs")
        s_list = []
        n = 0
        for fna in fasta_files:
            f = fna.rsplit(".",1)
            if f[1] == 'fa' or f[1] == 'fna':
                fna = folder + "vRhyme_split_runs/" + fna
                n += 1
                if prodigal_split == False:
                    s = subprocess.Popen(f'{vRhyme_path}/scripts/kmer_nt_stuff_threader.py {fna} {folder} vRhyme_split_runs/{n} False', shell=True)
                else:
                    s = subprocess.Popen(f'{vRhyme_path}/scripts/kmer_nt_stuff_threader.py {fna} {folder} vRhyme_split_runs/{n} True', shell=True)
                s_list.append(s)
        for item in s_list:
            item.wait()

        for i in range(1,n+1):
            with open(folder + "vRhyme_split_runs/" + str(i) + "_pickle.sav", 'rb') as read_i:
                gc,cpg,skew,zom,tud,nt = pickle.load(read_i)

            info.data_dict_gc.update(gc)
            info.data_dict_cpg.update(cpg)
            info.data_dict_skew.update(skew)
            info.data_dict_zom.update(zom)
            info.data_dict_tud.update(tud)
            info.data_dict_nt.update(nt)
            subprocess.run("rm " + folder + "vRhyme_split_runs/" + str(i) + "_pickle.sav 2> /dev/null", shell=True)

        gc = None
        cpg = None
        skew = None
        zom = None
        tud = None
        nt = None

    pair_genomes = None

    log_time = str(round((time.time() - float(starting_time))/60,2))
    blanks = ''.join([' '] * (14-len(log_time)))
    logging.info("%s%sPerforming pairwise distance calculations" % (log_time,blanks))
    distances, pairs_machine, cohen_machine = distance_stuff.distance_pairs(folder, base, pairs, cohen_list, max_gc, min_kmer, mapper) #new_pairs

    info.data_dict_gc = None
    info.data_dict_cpg = None
    info.data_dict_skew = None
    info.data_dict_zom = None
    info.data_dict_tud = None
    info.data_dict_nt = None
    info.data_dict_codon = None
    cohen_list = None
    pairs = None

    if pairs_machine == []:
        log_time = str(round((time.time() - float(starting_time))/60,2))
        blanks = ''.join([' '] * (14-len(log_time)))
        logging.info("%s%sNo possible bins detected" % (log_time,blanks))

    else:
        log_time = str(round((time.time() - float(starting_time))/60,2))
        blanks = ''.join([' '] * (14-len(log_time)))
        logging.info("%s%sPerforming machine learning classification" % (log_time,blanks))
        net_data = machine_stuff.machine_stuff(distances, vRhyme_path, presets, model_method, pairs_machine, cohen_machine, iterations)
        cohen_machine = None
        pairs_machine = None

        log_time = str(round((time.time() - float(starting_time))/60,2))
        blanks = ''.join([' '] * (14-len(log_time)))
        logging.info("%s%sGenerating networks of bins" % (log_time,blanks))
        s_set = set()
        opened = 0
        for n in range(0,iterations):
            f = folder + str(n) + "_network_pickle.sav"
            with open(f, 'wb') as save_net:
                pickle.dump(net_data[n], save_net)

            s = subprocess.Popen(f'{vRhyme_path}/scripts/network_stuff_threader.py {f} {folder} {bin_size} {refine_m} {max_edges} {n}', shell=True)
            s_set.add(s)
            opened += 1
            if opened == int(threads):
                item = looper(s_set)
                opened -= 1
                s_set.remove(item)
        for item in s_set:
            item.wait()

        #################

        log_time = str(round((time.time() - float(starting_time))/60,2))
        blanks = ''.join([' '] * (14-len(log_time)))
        logging.info("%s%sExtracting information for all possible unique bins" % (log_time,blanks))
        uniques, total_seqs, total_bins = score_stuff.uniques_stuff(folder)

        if uniques != None:
            #
            log_time = str(round((time.time() - float(starting_time))/60,2))
            blanks = ''.join([' '] * (14-len(log_time)))
            logging.info("%s%sRunning mmseqs2 linclust for bin redundancy scoring" % (log_time,blanks))
            prots_dict = extract_stuff.replace_mapper(in_prots, folder, keep)
            linclust_stuff.linclust(folder, threads)
            keep = None
            #
            log_time = str(round((time.time() - float(starting_time))/60,2))
            blanks = ''.join([' '] * (14-len(log_time)))
            logging.info("%s%sParsing linclust protein clusters" % (log_time,blanks))
            linclust_master = linclust_stuff.parse_linclust(folder)
            #
            log_time = str(round((time.time() - float(starting_time))/60,2))
            blanks = ''.join([' '] * (14-len(log_time)))
            logging.info("%s%sScoring all possible unique bins" % (log_time,blanks))
            redundancy_dict, proteins_per_bin, uniques_info, total_bins, total_seqs = score_stuff.score_stuff(uniques, iterations, prots_dict, linclust_master, red_cutoff, total_bins, total_seqs)
            #
            log_time = str(round((time.time() - float(starting_time))/60,2))
            blanks = ''.join([' '] * (14-len(log_time)))
            logging.info("%s%sIdentifying best set of bins from %s iterations" % (log_time,blanks, iterations))
            best, scoring = score_stuff.best_bin(folder, iterations, total_seqs, redundancy_dict, total_bins, proteins_per_bin, length_keep)
            #
            log_time = str(round((time.time() - float(starting_time))/60,2))
            blanks = ''.join([' '] * (14-len(log_time)))
            logging.info("%s%sExtracting binning summary statistics for each iteration" % (log_time,blanks))
            final, binned_seqs, bins_count, best_score, binned_prots, binned_redundancy = score_stuff.final_bin(folder, best, scoring, mapper, uniques, uniques_info)
            uniques = None
            uniques_redundancy = None
            uniques_proteins = None
            #
            log_time = str(round((time.time() - float(starting_time))/60,2))
            blanks = ''.join([' '] * (14-len(log_time)))
            logging.info("%s%sWriting finalized bin sequences to individual fasta files" % (log_time,blanks))
            extract_stuff.final_fasta(in_fasta, in_genes, in_prots, folder, prefix, final)
        else:
            log_time = str(round((time.time() - float(starting_time))/60,2))
            blanks = ''.join([' '] * (14-len(log_time)))
            logging.info("%s%sNo possible bins detected" % (log_time,blanks))


s_list = []
if distances != None:
    subprocess.run(f"paste {folder}log_vRhyme_distance_names.tsv {distances} > {folder}vRhyme_machine_distances.tsv 2> /dev/null", shell=True)
    s = subprocess.Popen(f"rm {folder}log_vRhyme_distance_names.tsv 2> /dev/null", shell=True)
    s_list.append(s)
    s = subprocess.Popen(f"rm {distances} 2> /dev/null", shell=True)
    s_list.append(s)
s = subprocess.Popen("rm " + folder + "keep_pickle.sav 2> /dev/null", shell=True)
s_list.append(s)
s = subprocess.Popen("rm -R " + folder + "vRhyme_linclust_clustering 2> /dev/null", shell=True)
s_list.append(s)
s = subprocess.Popen("rm -R " + folder + "vRhyme_split_runs 2> /dev/null", shell=True)
s_list.append(s)
check_list = os.listdir(folder + "vRhyme_bam_files")
if len(check_list) == 0:
    s = subprocess.Popen("rm -R " + folder + "vRhyme_bam_files", shell=True)
    s_list.append(s)
check_list = os.listdir(folder + "vRhyme_sam_files")
if len(check_list) == 0:
    s = subprocess.Popen("rm -R " + folder + "vRhyme_sam_files", shell=True)
    s_list.append(s)
check_list = os.listdir(folder + "vRhyme_coverage_files")
if len(check_list) == 0:
    s = subprocess.Popen("rm -R " + folder + "vRhyme_coverage_files", shell=True)
    s_list.append(s)

for s in s_list:
    s.wait()

log_time = str(round((time.time() - float(starting_time))/60,2))
blanks = ''.join([' '] * (14-len(log_time)))
logging.info("%s%svRhyme binning complete\n" % (log_time,blanks))
#
mem_self = float(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)
mem_child = float(resource.getrusage(resource.RUSAGE_CHILDREN).ru_maxrss)
max_mem = float(max([mem_self, mem_child])/1e6)
mem = round(max_mem,2)
try:
    fraction_binned = round((binned_seqs/length_keep)*100,1)
    fraction_redundancy = round((binned_redundancy/binned_prots)*100,1)
except Exception:
    fraction_binned = 0
    fraction_redundancy = 0
logging.info(f"Memory usage:       {mem}")
logging.info(f"Runtime (min):      {log_time}")
logging.info(f"Bins generated:     {bins_count}")
logging.info(f"Binned sequences:   {binned_seqs} ({fraction_binned}%)")
logging.info(f"Input sequences:    {length_keep}")
logging.info(f"Binned proteins:    {binned_prots}")
logging.info(f"Redundant proteins: {binned_redundancy} ({fraction_redundancy}%)")
logging.info(f"Best iteration:     {final}")
logging.info(f"vRhyme score:       {best_score}")
logging.info("")
logging.info("")
logging.info("______________________________________________________________________")
logging.info("")
logging.info('             ## ## ## ##                                              ')
logging.info('             ##       ##  ##      ##     ##    ## ## ##     # ## ##   ')
logging.info('##       ##  ##       ##  ##       ##    ##  ##   ##   ##  ##      #  ')
logging.info(' ##     ##   ##     ##    ##         ## ##   ##   ##   ##  ## ## ##   ')
logging.info('  ##   ##    ## ####      ## ## ##     ##    ##   ##   ##  ##         ')
logging.info('   ## ##     ##   ##      ##    ##    ##     ##   ##   ##  ##         ')
logging.info('    ###      ##     ##    ##    ##   ##      ##   ##   ##   ## ## ##  ')
logging.info("______________________________________________________________________")
logging.info("")
logging.info("")


#
#
#
